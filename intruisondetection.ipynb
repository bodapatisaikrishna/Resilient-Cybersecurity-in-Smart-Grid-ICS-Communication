{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae59919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.spatial.distance import euclidean\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging for IDS monitoring\n",
    "logging.basicConfig(filename='ids_log.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class IntrusionDetectionSystem:\n",
    "    def __init__(self):\n",
    "        self.clf = None\n",
    "        self.scaler = None\n",
    "        self.imputer = None\n",
    "        self.feature_columns = None\n",
    "\n",
    "    # Debugging function to check shape alignment\n",
    "    def _debug_shape(self, df, name=\"DataFrame\"):\n",
    "        print(f\"{name} shape: {df.shape}\")\n",
    "        logging.info(f\"{name} shape: {df.shape}\")\n",
    "\n",
    "    # Function to safely load CSV files\n",
    "    def _safe_load_csv(self, filepath, description=\"data\"):\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, delimiter=';', encoding='utf-8')\n",
    "            logging.info(f\"Successfully loaded {description} from {filepath}\")\n",
    "            return df\n",
    "        except pd.errors.ParserError as e:\n",
    "            logging.warning(f\"Error loading {filepath}: {e}. Skipping problematic rows...\")\n",
    "            return pd.read_csv(filepath, on_bad_lines='skip', delimiter=';', encoding='utf-8')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load {filepath}: {e}\")\n",
    "            raise\n",
    "\n",
    "    # Function to clean mixed or improperly parsed columns\n",
    "    def _clean_data_columns(self, df):\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        return df\n",
    "\n",
    "    # Transformation function for normal traffic (simulating benign variation)\n",
    "    def _transform_normal_data(self, data, noise_level=0.5):\n",
    "        noise = np.random.normal(0, noise_level, data.shape)\n",
    "        scaled_data = data * np.random.uniform(0.5, 1.5, data.shape)\n",
    "        transformed_data = scaled_data + noise\n",
    "        return transformed_data\n",
    "\n",
    "    # Train the IDS model\n",
    "    def train(self, normal_path, attack_paths):\n",
    "        # 1. Load normal dataset\n",
    "        normal = self._safe_load_csv(normal_path, \"normal traffic\")\n",
    "        normal['label'] = 0\n",
    "\n",
    "        # 2. Load and combine all attack datasets\n",
    "        attack_dfs = []\n",
    "        for attack_path in attack_paths:\n",
    "            attack_df = self._safe_load_csv(attack_path, f\"attack traffic from {attack_path.split('/')[-1]}\")\n",
    "            attack_df['label'] = 1\n",
    "            attack_dfs.append(attack_df)\n",
    "        \n",
    "        # Combine all attack data\n",
    "        attack = pd.concat(attack_dfs, ignore_index=True)\n",
    "        \n",
    "        # Combine normal and attack data\n",
    "        data = pd.concat([normal, attack], ignore_index=True)\n",
    "        self._debug_shape(data, \"Combined Data\")\n",
    "\n",
    "        # Clean data\n",
    "        data = self._clean_data_columns(data)\n",
    "\n",
    "        # 3. Transform normal traffic to introduce variation\n",
    "        normal_features = data[data['label'] == 0].drop(columns=['label'])\n",
    "        self._debug_shape(normal_features, \"Normal Features Before Transformation\")\n",
    "\n",
    "        transformed_normal_features = self._transform_normal_data(normal_features.to_numpy(), noise_level=0.5)\n",
    "        transformed_normal = pd.DataFrame(transformed_normal_features, columns=normal_features.columns)\n",
    "        transformed_normal['label'] = 0\n",
    "        self._debug_shape(transformed_normal, \"Transformed Normal Data\")\n",
    "\n",
    "        # Combine transformed normal with attack data\n",
    "        attack_data = data[data['label'] == 1]\n",
    "        data_transformed = pd.concat([transformed_normal, attack_data], ignore_index=True)\n",
    "        self._debug_shape(data_transformed, \"Data Transformed After Transformation\")\n",
    "\n",
    "        # Ensure consistency in cleaning\n",
    "        data_transformed = self._clean_data_columns(data_transformed)\n",
    "\n",
    "        # 4. Handle missing values\n",
    "        non_nan_columns = data_transformed.columns[data_transformed.notna().any()].tolist()\n",
    "        data_transformed = data_transformed[non_nan_columns]\n",
    "        self._debug_shape(data_transformed, \"Data Transformed After Dropping NaN Columns\")\n",
    "\n",
    "        # Separate features and labels\n",
    "        features_transformed = data_transformed.drop(columns=['label'])\n",
    "        label_transformed = data_transformed['label']\n",
    "        self._debug_shape(features_transformed, \"Features Transformed\")\n",
    "\n",
    "        # Impute missing values\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        features_cleaned = self.imputer.fit_transform(features_transformed)\n",
    "        features_cleaned_df = pd.DataFrame(features_cleaned, columns=features_transformed.columns)\n",
    "        features_cleaned_df['label'] = label_transformed.reset_index(drop=True)\n",
    "        self._debug_shape(features_cleaned_df, \"Features Cleaned\")\n",
    "\n",
    "        # 5. Preprocessing\n",
    "        self.scaler = StandardScaler()\n",
    "        numerical_features = [col for col in features_cleaned_df.columns if col != 'label']\n",
    "        features_cleaned_df[numerical_features] = self.scaler.fit_transform(features_cleaned_df[numerical_features])\n",
    "        features_cleaned_df['label'] = features_cleaned_df['label'].astype(int)\n",
    "\n",
    "        # Store feature columns for monitoring\n",
    "        self.feature_columns = numerical_features\n",
    "\n",
    "        # Calculate Euclidean distances for transformation validation\n",
    "        normal_features_clean = np.nan_to_num(normal_features.to_numpy(), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        transformed_normal_features_clean = np.nan_to_num(transformed_normal_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        euclidean_distances = [euclidean(original, transformed) \n",
    "                              for original, transformed in zip(normal_features_clean, transformed_normal_features_clean)]\n",
    "        euclidean_mean = np.mean(euclidean_distances)\n",
    "        euclidean_std = np.std(euclidean_distances)\n",
    "        print(f\"Euclidean Distance: Mean = {euclidean_mean}, Std Dev = {euclidean_std}\")\n",
    "        logging.info(f\"Euclidean Distance: Mean = {euclidean_mean}, Std Dev = {euclidean_std}\")\n",
    "\n",
    "        # Plot Euclidean Distance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(euclidean_distances, kde=True, bins=50, color='green', label='Euclidean Distance')\n",
    "        plt.title('Euclidean Distance Before and After Transformation', fontsize=16)\n",
    "        plt.xlabel('Euclidean Distance', fontsize=14)\n",
    "        plt.ylabel('Frequency', fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.show()\n",
    "\n",
    "        # 6. Split data\n",
    "        X = features_cleaned_df.drop(columns=['label'])\n",
    "        y = features_cleaned_df['label']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        self._debug_shape(X_train, \"X_train\")\n",
    "        self._debug_shape(X_test, \"X_test\")\n",
    "\n",
    "        print(\"Class distribution in y_train:\")\n",
    "        print(y_train.value_counts())\n",
    "\n",
    "        # 7. Handle imbalance with SMOTE\n",
    "        majority_class_count = y_train.value_counts().max()\n",
    "        smote = SMOTE(sampling_strategy={1: majority_class_count}, random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "        self._debug_shape(X_train_resampled, \"X_train_resampled\")\n",
    "\n",
    "        print(\"Class distribution after SMOTE:\")\n",
    "        print(y_train_resampled.value_counts())\n",
    "\n",
    "        # 8. Train XGBoost classifier\n",
    "        self.clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "        self.clf.fit(X_train_resampled, y_train_resampled)\n",
    "        logging.info(\"IDS model training completed.\")\n",
    "\n",
    "        # 9. Evaluate the model\n",
    "        self._evaluate_model(X_test, y_test)\n",
    "\n",
    "    # Evaluation function\n",
    "    def _evaluate_model(self, X_test, y_test):\n",
    "        y_pred = self.clf.predict(X_test)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # ROC Curve\n",
    "        y_pred_prob = self.clf.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        logging.info(f\"Model Accuracy: {accuracy:.2f}, ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "        # Confusion Matrix Plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Intrusion'], yticklabels=['Benign', 'Intrusion'])\n",
    "        plt.title('Confusion Matrix', fontsize=16)\n",
    "        plt.xlabel('Predicted', fontsize=14)\n",
    "        plt.ylabel('True', fontsize=14)\n",
    "        plt.show()\n",
    "\n",
    "        # Metrics Plot\n",
    "        metrics_df = pd.DataFrame(report).transpose().iloc[:-1, :3]\n",
    "        ax = metrics_df.plot(kind='bar', figsize=(14, 8), color=['blue', 'green', 'red'])\n",
    "        plt.title('Classification Metrics', fontsize=30)\n",
    "        plt.xlabel('Metrics', fontsize=30)\n",
    "        plt.ylabel('Score', fontsize=30)\n",
    "        plt.yticks(fontsize=30)\n",
    "        plt.legend(fontsize=30)\n",
    "        labels = [label.get_text() for label in ax.get_xticklabels()]\n",
    "        updated_labels = [f\"{label}\" for label in labels]\n",
    "        ax.set_xticklabels(updated_labels, fontsize=30, rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # ROC Curve Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\n",
    "        plt.xlabel('False Positive Rate', fontsize=14)\n",
    "        plt.ylabel('True Positive Rate', fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.show()\n",
    "\n",
    "    # Real-time monitoring with combined normal and attack data\n",
    "    def monitor_traffic(self, normal_path, attack_paths):\n",
    "        if not all([self.clf, self.scaler, self.imputer]):\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "\n",
    "        # Load normal data\n",
    "        normal = self._safe_load_csv(normal_path, \"normal traffic for monitoring\")\n",
    "        normal['label'] = 0\n",
    "\n",
    "        # Load and combine attack data\n",
    "        attack_dfs = []\n",
    "        for attack_path in attack_paths:\n",
    "            attack_df = self._safe_load_csv(attack_path, f\"attack traffic for monitoring from {attack_path.split('/')[-1]}\")\n",
    "            attack_df['label'] = 1\n",
    "            attack_dfs.append(attack_df)\n",
    "        \n",
    "        attack = pd.concat(attack_dfs, ignore_index=True)\n",
    "\n",
    "        # Combine normal and attack data for monitoring\n",
    "        new_data = pd.concat([normal, attack], ignore_index=True)\n",
    "        self._debug_shape(new_data, \"Combined Monitoring Data\")\n",
    "\n",
    "        # Clean the data\n",
    "        new_data = self._clean_data_columns(new_data)\n",
    "        \n",
    "        # Ensure feature alignment using stored feature columns from training\n",
    "        if self.feature_columns:\n",
    "            available_features = [col for col in self.feature_columns if col in new_data.columns]\n",
    "            if not available_features:\n",
    "                raise ValueError(\"No matching features found in new data compared to training data.\")\n",
    "            features = new_data[available_features]\n",
    "        else:\n",
    "            raise ValueError(\"Feature columns not set during training.\")\n",
    "\n",
    "        # Impute missing values\n",
    "        features_imputed = self.imputer.transform(features)\n",
    "        features_df = pd.DataFrame(features_imputed, columns=features.columns)\n",
    "        \n",
    "        # Scale the features\n",
    "        features_scaled = self.scaler.transform(features_df)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.clf.predict(features_scaled)\n",
    "        probabilities = self.clf.predict_proba(features_scaled)[:, 1]\n",
    "\n",
    "        # Counter for intrusions detected\n",
    "        intrusion_count = 0\n",
    "\n",
    "        # Log and print results with actual labels for comparison\n",
    "        actual_labels = new_data['label'].values\n",
    "        for idx, (pred, prob, actual) in enumerate(zip(predictions, probabilities, actual_labels)):\n",
    "            timestamp = datetime.now()\n",
    "            confidence = prob if pred == 1 else 1 - prob\n",
    "            pred_label = \"Intrusion\" if pred == 1 else \"Benign\"\n",
    "            actual_label = \"Intrusion\" if actual == 1 else \"Benign\"\n",
    "            \n",
    "            if pred == 1:\n",
    "                intrusion_count += 1\n",
    "                logging.warning(f\"Packet {idx} at {timestamp}: Predicted {pred_label} (Confidence: {confidence:.2f}), Actual: {actual_label}\")\n",
    "                print(f\"Packet {idx} at {timestamp}: Predicted {pred_label} (Confidence: {confidence:.2f}), Actual: {actual_label}\")\n",
    "            else:\n",
    "                logging.info(f\"Packet {idx} at {timestamp}: Predicted {pred_label} (Confidence: {confidence:.2f}), Actual: {actual_label}\")\n",
    "                print(f\"Packet {idx} at {timestamp}: Predicted {pred_label} (Confidence: {confidence:.2f}), Actual: {actual_label}\")\n",
    "\n",
    "        # Print total number of intrusions detected\n",
    "        print(f\"\\nTotal number of intrusions detected: {intrusion_count}\")\n",
    "        logging.info(f\"Total number of intrusions detected: {intrusion_count}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize IDS\n",
    "    ids = IntrusionDetectionSystem()\n",
    "\n",
    "    # Paths to datasets\n",
    "    normal_path = \"/Users/bodapati/Desktop/Eoc project/ICS Dataset for Smart Grid Anomaly Detection/ics-dataset-for-smart-grids/but-iec104-i/normal-traffic.csv\"\n",
    "    \n",
    "    attack_paths = [\n",
    "        \"/Users/bodapati/Desktop/Eoc project/ICS Dataset for Smart Grid Anomaly Detection/ics-dataset-for-smart-grids/but-iec104-i/switching-attack.csv\",\n",
    "        \"/Users/bodapati/Desktop/Eoc project/ICS Dataset for Smart Grid Anomaly Detection/ics-dataset-for-smart-grids/but-iec104-i/connection-loss.csv\",\n",
    "        \"/Users/bodapati/Desktop/Eoc project/ICS Dataset for Smart Grid Anomaly Detection/ics-dataset-for-smart-grids/but-iec104-i/dos-attack.csv\",\n",
    "        \"/Users/bodapati/Desktop/Eoc project/ICS Dataset for Smart Grid Anomaly Detection/ics-dataset-for-smart-grids/but-iec104-i/injection-attack.csv\",\n",
    "        \"/Users/bodapati/Desktop/Eoc project/ICS Dataset for Smart Grid Anomaly Detection/ics-dataset-for-smart-grids/but-iec104-i/rogue-device.csv\",\n",
    "        \"/Users/bodapati/Desktop/Eoc project/ICS Dataset for Smart Grid Anomaly Detection/ics-dataset-for-smart-grids/but-iec104-i/scanning-attack.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Train the IDS model with all attack datasets\n",
    "    ids.train(normal_path, attack_paths)\n",
    "\n",
    "    # Perform real-time monitoring with combined normal and attack data\n",
    "    ids.monitor_traffic(normal_path, attack_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
